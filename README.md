# ğŸ§  Enhanced Q&A Chatbot (OpenAI & Ollama)

An interactive **Questionâ€“Answer Chatbot** built using **Streamlit + LangChain** that supports:

- â˜ï¸ **Cloud-based LLMs (OpenAI GPT models)**
- ğŸ–¥ï¸ **Local open-source LLMs (Ollama â€“ Mistral)**
- ğŸ” **LangSmith tracing & monitoring**
- ğŸ›ï¸ Model & generation parameter controls

This project is ideal for learning **Generative AI application development** and can be extended to document QA, agents, and RAG systems.

---

## ğŸš€ Features

- Streamlit-based web UI
- Switch between **OpenAI** and **Ollama** models
- Adjustable temperature & max tokens
- Secure API key handling
- LangChain Expression Language (LCEL)
- LangSmith tracking enabled
- Fully modular & beginner-friendly

---

## ğŸ§° Tech Stack

- **Python**
- **Streamlit**
- **LangChain**
- **OpenAI API**
- **Ollama (local LLMs)**
- **LangSmith**
- **dotenv**

---

## ğŸ“‚ Project Structure

1-QA-chatbot/
â”‚
â”œâ”€â”€ main.py # Main Streamlit application
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env # API keys (not pushed to GitHub)
â”œâ”€â”€ README.md
â”œâ”€â”€ venv/
â””â”€â”€ screenshots/ # Output screenshots (optional)


---

## ğŸ”‘ Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
LANGCHAIN_API_KEY=lsv2_xxxxxxxxxxxxx


How to Run (OpenAI Mode)
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
streamlit run main.py

How to Run (Ollama Mode â€“ Local)
1. Install Ollama

ğŸ‘‰ https://ollama.com

2. Start Ollama server
ollama serve

3. Pull model
ollama pull mistral

4. Run the app
streamlit run main.py


